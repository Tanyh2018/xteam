---
title: 305 - Machine Learning 
---

**Contents**
* TOC
{:toc}


## Machine Learning 
### Support Vector Machines(SVM)
Support Vector Machine is used to classify events, by dividing the dataset into two categories. As a typical algorithm of machine learning, it consists a vector-like training set 
<img src="http://chart.googleapis.com/chart?cht=tx&chl= ${(\mathbf{x_{1}},y_{1}),(\mathbf{x_{2}},y_{2}),...,(\mathbf{x_{i}},y_{i})}$" style="border:none;">
Here,<img src="http://chart.googleapis.com/chart?cht=tx&chl=$y_{i}\in {+1,-1}$" style="border:none;">
  ,represents the category label. <img src="http://chart.googleapis.com/chart?cht=tx&chl=$x_{i}\in \mathbb{R}^{d}$" style="border:none;">is the feature vector in vector space.
First, SVM constructs the sample data as d-dimensional feature vectors. Then the problem can be solved in this vector space by creating a so-called “hyperplane” which is the dividing line between two classes. The support vector is the vector which is closest to the hyperplane in vector space. 
A d-dimensional hyperplane can be expressed as
<img src="http://chart.googleapis.com/chart?cht=tx&chl=$$\mathbf{w}\circ\mathbf{x}+b$$" style="border:none;">
 
w is a d-dimensional vector which determines the presence of hyperplane.
The perpendicular distance of two support vectors divided by hyperplane is called margin, equivalent to $||w||^-1$.To optimize the results of classification is to maximize the margin of categories. In other words, our classification task is to find a minimum of $||w||$ in reality.
Based on Lagrange multiplier method, we can successfully construct a Lagrangian and the problem transforms to
min⁡  L=1/2∥w∥^2-∑_(i=1)^N▒〖α_i (y_i (w⋅x_i+b)-1)〗
s.t.  ∑_(i=1)^N▒〖α_i y_i 〗=0
Here in Lagrangian , α_i is multiplier, and 0≤α≤C, C is called penalty parameter. The constraint above ensures the KKT conditions.
By solving this optimization problem numerically, we get the optimal solution α^* and w^*=∑_(i=1)^N▒α_i^*  y_i x_i  ,b^*=y_i-∑_(i=1)^N▒〖α_i^* y_i (x_i⋅x_j)〗. The index j corresponds to α_j>0 which is a random component of α^*.
Finally, the decision function of classification is given as m ̃=sign(w^*⋅x_i+b^* )=y_i.

### Support Vector Machines with a quantum kernel method
The reason for using the kernel method is that the vectors we construct are not linearly separable in their own feature space. In this case, the hyperplane is hard to express.
An efficient way to deal with this occasion is to consider a feature map from present feature space to a high-dimension space, which is called kernel method in classical computing. Then the data shall be linearly separable. Researchers has founded that we can use the inner product of feature map to construct the classifier without knowing the specific mathematical expression. Generally, we call the inner product as kernel. However, due to the particular quantity for computing, it is nearly inaccessible to accomplish the mission. This is called “Curse of Dimensionality” by Richard E. Bellman.
On the other hand, quantum space, also called “Hilbert space”, has a distinctive excellence to deal with the huge computing quantity. The probability of quantum state makes it possible to map any feature vectors in quantum space at the same time, which undoubtedly reduce the time complexity. Combining with quantum kernel method, it is easy to use SVM by mapping the data sample vectors into Hilbert space. 


 



